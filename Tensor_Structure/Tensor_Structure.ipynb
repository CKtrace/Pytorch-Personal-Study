{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed62d69f",
   "metadata": {},
   "source": [
    "# Tensor Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571200e1",
   "metadata": {},
   "source": [
    "## [Python List] vs [Torch Tensor]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "720d0530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "[1.0, 2.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Python List\n",
    "\n",
    "a = [1.0, 2.0, 1.0]\n",
    "\n",
    "print(a[0])\n",
    "print(a[2])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1987d887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1.])\n",
      "tensor(1.)\n",
      "1.0\n",
      "--------------------\n",
      "tensor([1., 1., 2.])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensor\n",
    "\n",
    "a = torch.ones(3)\n",
    "\n",
    "print(a)\n",
    "print(a[1])\n",
    "print(float(a[1]))\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "a[2] = 2.0\n",
    "print(a)\n",
    "\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "035e848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n",
      "torch.Size([3, 2])\n",
      "--------------------\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "--------------------\n",
      "tensor(1.)\n",
      "tensor([4., 1.])\n"
     ]
    }
   ],
   "source": [
    "ts = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "\n",
    "print(ts)\n",
    "print(ts.shape)\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "ts = torch.zeros(3, 2)\n",
    "print(ts)\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "ts = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "print(ts[0, 1])\n",
    "print(ts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280d1bcb",
   "metadata": {},
   "source": [
    "## BroadCasting Case 1) Not Using torch.einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "370b2338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1.5983, -0.3484,  0.5655,  0.3301,  1.1145],\n",
      "         [-0.4947, -0.7328,  0.3209, -0.0710,  1.2660],\n",
      "         [ 0.2566, -0.5516,  0.6858,  2.2369,  0.2389],\n",
      "         [-0.3658, -1.5018,  0.0093,  1.0337,  0.9548],\n",
      "         [-0.6740, -0.4026,  0.7824,  0.8234, -1.5986]],\n",
      "\n",
      "        [[ 0.4735, -0.2425, -0.8112,  0.0550, -0.2995],\n",
      "         [-0.4127, -0.4388, -1.1521, -0.8011,  0.3785],\n",
      "         [-1.8889, -2.7838,  0.1517,  0.6229, -0.4998],\n",
      "         [-0.0773,  1.6237,  1.0182, -0.3051,  1.2892],\n",
      "         [-0.8027, -0.0293, -1.7940,  0.5115, -0.1719]],\n",
      "\n",
      "        [[-0.7189,  0.9472,  0.8887,  0.1411, -0.3666],\n",
      "         [-0.6374,  1.1175, -1.8483, -0.2069, -0.4688],\n",
      "         [ 0.7709, -0.8909,  0.3729,  1.4639, -1.9464],\n",
      "         [ 0.6226,  1.8907,  0.4442,  0.9469,  0.2237],\n",
      "         [ 0.7057,  1.3127,  0.1154, -0.1406,  0.7674]]])\n",
      "torch.Size([3, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "img_t = torch.randn(3, 5, 5)\n",
    "print(img_t)\n",
    "print(img_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9d55f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([0.2126, 0.7152, 0.0722])\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d20fbf29",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-1.8390e+00, -6.1058e-02, -2.3992e+00,  3.8978e-01,  7.2499e-01],\n",
       "          [ 1.6839e+00, -1.8949e+00,  3.6814e-01,  2.1233e-01, -2.1607e-01],\n",
       "          [-1.0920e+00, -2.0470e-01, -2.7246e-01,  8.0659e-01,  7.5569e-01],\n",
       "          [ 2.6040e-01,  1.5757e-01, -6.0189e-01, -3.4326e-01, -2.9322e-01],\n",
       "          [ 9.5835e-01,  2.0278e-01,  9.0836e-02,  4.1205e-01,  8.7899e-01]],\n",
       "\n",
       "         [[-7.9365e-01, -3.0581e-01, -3.8241e-01,  1.6052e+00,  4.6246e-01],\n",
       "          [ 1.6527e+00,  5.4450e-02,  4.3255e-01,  1.3183e+00,  1.0210e-01],\n",
       "          [ 4.9393e-01,  1.3192e-01,  1.3010e+00,  5.9772e-01,  2.2005e+00],\n",
       "          [-1.2698e+00, -5.3810e-01,  5.0625e-01,  1.0178e+00,  1.3289e+00],\n",
       "          [ 4.7395e-01,  8.1816e-01, -1.4656e+00, -1.7423e-01, -1.0069e+00]],\n",
       "\n",
       "         [[ 1.3774e+00,  2.0508e+00,  1.5535e+00,  1.6857e-02, -4.6688e-01],\n",
       "          [-1.4446e+00, -3.4252e-01, -5.9673e-01,  5.2440e-03,  2.6123e-01],\n",
       "          [-8.9691e-01, -1.0433e+00,  2.3154e-01, -7.9168e-02,  1.0703e+00],\n",
       "          [-8.3009e-01, -5.8811e-01,  1.2269e+00,  1.4723e+00, -9.0581e-01],\n",
       "          [ 2.3837e-01,  2.0372e+00, -7.8326e-01, -7.7472e-03,  3.1948e-01]]],\n",
       "\n",
       "\n",
       "        [[[ 5.5886e-01,  9.2966e-01,  8.6317e-01,  6.1309e-01,  1.9180e+00],\n",
       "          [ 1.2008e+00, -5.3078e-01,  2.1377e+00, -3.2677e-02, -1.4597e+00],\n",
       "          [ 5.7661e-02, -7.1509e-01,  4.2829e-01, -6.7170e-01, -1.9141e+00],\n",
       "          [ 1.8980e-02,  1.5791e+00,  6.2228e-01,  3.0274e-01, -3.6720e-01],\n",
       "          [-8.4814e-01, -1.9531e-01,  5.0201e-02, -1.2810e+00, -6.2288e-01]],\n",
       "\n",
       "         [[ 1.4232e+00,  5.8610e-01,  2.2274e+00, -1.0463e+00,  9.2158e-01],\n",
       "          [-1.2160e+00, -2.8694e-01,  8.4271e-02,  2.1891e-03,  6.0872e-01],\n",
       "          [-4.3967e-01,  4.4622e-01, -8.1341e-01,  1.4252e+00,  1.4121e+00],\n",
       "          [-1.2030e+00,  1.7243e+00,  3.7410e-01, -4.2618e-01,  4.6995e-01],\n",
       "          [ 1.5750e+00,  1.5182e+00, -7.8844e-01,  1.8983e-01,  1.7244e+00]],\n",
       "\n",
       "         [[ 3.1458e-02,  4.2683e-01,  1.5043e+00, -1.5455e+00,  9.7076e-01],\n",
       "          [-2.0997e-01, -2.0384e-01,  1.5310e+00,  1.3286e-01,  2.8665e-01],\n",
       "          [ 1.0859e+00, -8.4262e-01, -1.3711e+00, -1.1649e+00,  1.2811e+00],\n",
       "          [-3.7335e-01, -4.5785e-01,  7.2541e-01,  3.6337e-01,  5.8786e-01],\n",
       "          [-7.9628e-02, -1.1603e+00, -7.8281e-01,  1.3738e+00, -5.6631e-01]]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_t = torch.randn(2, 3, 5, 5)\n",
    "batch_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f5f354e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4510,  0.1187,  0.2143,  0.1754,  0.1495],\n",
      "        [-0.5150, -0.0181, -0.8932, -0.3597,  0.3919],\n",
      "        [-0.2871, -1.4088,  0.4035,  1.4412, -0.7358],\n",
      "        [ 0.0598,  0.6709,  0.4906,  0.5585,  0.8225],\n",
      "        [-0.2570,  0.2936, -0.2988,  0.3981, -0.3344]])\n",
      "tensor([[[-0.4184,  0.5613, -0.4094,  0.6706,  0.2402],\n",
      "         [ 0.6307, -0.7277,  0.0680,  0.5120,  0.0491],\n",
      "         [-0.4983, -0.3720,  0.4200,  0.4417,  1.3422],\n",
      "         [-0.6132, -0.3229,  0.3771,  0.7156,  0.0433],\n",
      "         [ 0.5569,  1.0194, -0.7193,  0.0767,  0.0639]],\n",
      "\n",
      "        [[ 0.6712,  0.6475,  1.5316, -0.6596,  1.2701],\n",
      "         [-0.0750, -0.3405,  1.2510,  0.0341, -0.1881],\n",
      "         [ 0.2346, -0.3705, -0.5854, -0.1371,  0.2597],\n",
      "         [-0.5191,  0.9485,  0.5739,  0.0800,  0.2302],\n",
      "         [ 0.2157,  0.0542, -0.5070,  0.0942,  0.1784]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), torch.Size([2, 5, 5]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_naive = img_t.mean(-3)\n",
    "batch_gray_naive = batch_t.mean(-3)\n",
    "\n",
    "print(img_gray_naive)\n",
    "print(batch_gray_naive)\n",
    "\n",
    "img_gray_naive.shape, batch_gray_naive.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0934a6f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3]) => torch.Size([3, 1, 1])\n",
      "tensor([[[0.2126]],\n",
      "\n",
      "        [[0.7152]],\n",
      "\n",
      "        [[0.0722]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.3980e-01, -7.4079e-02,  1.2023e-01,  7.0175e-02,  2.3693e-01],\n",
       "         [-1.0517e-01, -1.5580e-01,  6.8221e-02, -1.5103e-02,  2.6915e-01],\n",
       "         [ 5.4563e-02, -1.1727e-01,  1.4580e-01,  4.7557e-01,  5.0788e-02],\n",
       "         [-7.7764e-02, -3.1929e-01,  1.9753e-03,  2.1976e-01,  2.0298e-01],\n",
       "         [-1.4329e-01, -8.5591e-02,  1.6633e-01,  1.7505e-01, -3.3986e-01]],\n",
       "\n",
       "        [[ 3.3868e-01, -1.7347e-01, -5.8019e-01,  3.9338e-02, -2.1418e-01],\n",
       "         [-2.9519e-01, -3.1386e-01, -8.2395e-01, -5.7294e-01,  2.7067e-01],\n",
       "         [-1.3509e+00, -1.9910e+00,  1.0852e-01,  4.4552e-01, -3.5745e-01],\n",
       "         [-5.5284e-02,  1.1613e+00,  7.2824e-01, -2.1824e-01,  9.2200e-01],\n",
       "         [-5.7411e-01, -2.0955e-02, -1.2831e+00,  3.6583e-01, -1.2297e-01]],\n",
       "\n",
       "        [[-5.1903e-02,  6.8390e-02,  6.4163e-02,  1.0187e-02, -2.6470e-02],\n",
       "         [-4.6023e-02,  8.0686e-02, -1.3345e-01, -1.4938e-02, -3.3851e-02],\n",
       "         [ 5.5659e-02, -6.4326e-02,  2.6920e-02,  1.0569e-01, -1.4053e-01],\n",
       "         [ 4.4949e-02,  1.3651e-01,  3.2070e-02,  6.8364e-02,  1.6149e-02],\n",
       "         [ 5.0951e-02,  9.4774e-02,  8.3330e-03, -1.0152e-02,  5.5408e-02]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unsqeezed_weights = weights.unsqueeze(-1).unsqueeze(-1) # BroadCasting\n",
    "print(f\"{weights.shape} => {unsqeezed_weights.shape}\") \n",
    "print(unsqeezed_weights)\n",
    "\n",
    "img_weights = (img_t * unsqeezed_weights)\n",
    "img_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8efc900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-3.9097e-01, -1.2981e-02, -5.1006e-01,  8.2868e-02,  1.5413e-01],\n",
       "          [ 3.5799e-01, -4.0285e-01,  7.8266e-02,  4.5140e-02, -4.5936e-02],\n",
       "          [-2.3216e-01, -4.3519e-02, -5.7925e-02,  1.7148e-01,  1.6066e-01],\n",
       "          [ 5.5362e-02,  3.3499e-02, -1.2796e-01, -7.2978e-02, -6.2339e-02],\n",
       "          [ 2.0375e-01,  4.3110e-02,  1.9312e-02,  8.7603e-02,  1.8687e-01]],\n",
       "\n",
       "         [[-5.6762e-01, -2.1872e-01, -2.7350e-01,  1.1480e+00,  3.3075e-01],\n",
       "          [ 1.1820e+00,  3.8943e-02,  3.0936e-01,  9.4286e-01,  7.3024e-02],\n",
       "          [ 3.5326e-01,  9.4350e-02,  9.3045e-01,  4.2749e-01,  1.5738e+00],\n",
       "          [-9.0817e-01, -3.8485e-01,  3.6207e-01,  7.2792e-01,  9.5045e-01],\n",
       "          [ 3.3897e-01,  5.8515e-01, -1.0482e+00, -1.2461e-01, -7.2011e-01]],\n",
       "\n",
       "         [[ 9.9446e-02,  1.4806e-01,  1.1216e-01,  1.2171e-03, -3.3709e-02],\n",
       "          [-1.0430e-01, -2.4730e-02, -4.3084e-02,  3.7862e-04,  1.8861e-02],\n",
       "          [-6.4757e-02, -7.5325e-02,  1.6717e-02, -5.7160e-03,  7.7274e-02],\n",
       "          [-5.9932e-02, -4.2461e-02,  8.8585e-02,  1.0630e-01, -6.5399e-02],\n",
       "          [ 1.7210e-02,  1.4709e-01, -5.6551e-02, -5.5935e-04,  2.3067e-02]]],\n",
       "\n",
       "\n",
       "        [[[ 1.1881e-01,  1.9765e-01,  1.8351e-01,  1.3034e-01,  4.0776e-01],\n",
       "          [ 2.5530e-01, -1.1284e-01,  4.5448e-01, -6.9470e-03, -3.1033e-01],\n",
       "          [ 1.2259e-02, -1.5203e-01,  9.1055e-02, -1.4280e-01, -4.0694e-01],\n",
       "          [ 4.0352e-03,  3.3572e-01,  1.3230e-01,  6.4363e-02, -7.8067e-02],\n",
       "          [-1.8031e-01, -4.1522e-02,  1.0673e-02, -2.7233e-01, -1.3242e-01]],\n",
       "\n",
       "         [[ 1.0179e+00,  4.1918e-01,  1.5931e+00, -7.4829e-01,  6.5911e-01],\n",
       "          [-8.6966e-01, -2.0522e-01,  6.0271e-02,  1.5656e-03,  4.3535e-01],\n",
       "          [-3.1445e-01,  3.1914e-01, -5.8175e-01,  1.0193e+00,  1.0099e+00],\n",
       "          [-8.6038e-01,  1.2332e+00,  2.6755e-01, -3.0481e-01,  3.3611e-01],\n",
       "          [ 1.1264e+00,  1.0858e+00, -5.6389e-01,  1.3577e-01,  1.2333e+00]],\n",
       "\n",
       "         [[ 2.2713e-03,  3.0817e-02,  1.0861e-01, -1.1159e-01,  7.0089e-02],\n",
       "          [-1.5160e-02, -1.4718e-02,  1.1054e-01,  9.5927e-03,  2.0696e-02],\n",
       "          [ 7.8402e-02, -6.0837e-02, -9.8994e-02, -8.4106e-02,  9.2494e-02],\n",
       "          [-2.6956e-02, -3.3057e-02,  5.2374e-02,  2.6235e-02,  4.2444e-02],\n",
       "          [-5.7492e-03, -8.3775e-02, -5.6519e-02,  9.9187e-02, -4.0888e-02]]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_weights = (batch_t * unsqeezed_weights)\n",
    "batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d599bac2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5])\n",
      "tensor([[[ 3.3980e-01, -7.4079e-02,  1.2023e-01,  7.0175e-02,  2.3693e-01],\n",
      "         [-1.0517e-01, -1.5580e-01,  6.8221e-02, -1.5103e-02,  2.6915e-01],\n",
      "         [ 5.4563e-02, -1.1727e-01,  1.4580e-01,  4.7557e-01,  5.0788e-02],\n",
      "         [-7.7764e-02, -3.1929e-01,  1.9753e-03,  2.1976e-01,  2.0298e-01],\n",
      "         [-1.4329e-01, -8.5591e-02,  1.6633e-01,  1.7505e-01, -3.3986e-01]],\n",
      "\n",
      "        [[ 3.3868e-01, -1.7347e-01, -5.8019e-01,  3.9338e-02, -2.1418e-01],\n",
      "         [-2.9519e-01, -3.1386e-01, -8.2395e-01, -5.7294e-01,  2.7067e-01],\n",
      "         [-1.3509e+00, -1.9910e+00,  1.0852e-01,  4.4552e-01, -3.5745e-01],\n",
      "         [-5.5284e-02,  1.1613e+00,  7.2824e-01, -2.1824e-01,  9.2200e-01],\n",
      "         [-5.7411e-01, -2.0955e-02, -1.2831e+00,  3.6583e-01, -1.2297e-01]],\n",
      "\n",
      "        [[-5.1903e-02,  6.8390e-02,  6.4163e-02,  1.0187e-02, -2.6470e-02],\n",
      "         [-4.6023e-02,  8.0686e-02, -1.3345e-01, -1.4938e-02, -3.3851e-02],\n",
      "         [ 5.5659e-02, -6.4326e-02,  2.6920e-02,  1.0569e-01, -1.4053e-01],\n",
      "         [ 4.4949e-02,  1.3651e-01,  3.2070e-02,  6.8364e-02,  1.6149e-02],\n",
      "         [ 5.0951e-02,  9.4774e-02,  8.3330e-03, -1.0152e-02,  5.5408e-02]]])\n",
      "****************************************************************************************************\n",
      "torch.Size([5, 5])\n",
      "tensor([[ 0.6266, -0.1792, -0.3958,  0.1197, -0.0037],\n",
      "        [-0.4464, -0.3890, -0.8892, -0.6030,  0.5060],\n",
      "        [-1.2407, -2.1726,  0.2812,  1.0268, -0.4472],\n",
      "        [-0.0881,  0.9785,  0.7623,  0.0699,  1.1411],\n",
      "        [-0.6665, -0.0118, -1.1084,  0.5307, -0.4074]])\n"
     ]
    }
   ],
   "source": [
    "img_gray_weighted = img_weights.sum(-3)\n",
    "\n",
    "print(img_weights.shape)\n",
    "print(img_weights)\n",
    "\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(img_gray_weighted.shape)\n",
    "print(img_gray_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc449224",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5, 5])\n",
      "tensor([[[[-3.9097e-01, -1.2981e-02, -5.1006e-01,  8.2868e-02,  1.5413e-01],\n",
      "          [ 3.5799e-01, -4.0285e-01,  7.8266e-02,  4.5140e-02, -4.5936e-02],\n",
      "          [-2.3216e-01, -4.3519e-02, -5.7925e-02,  1.7148e-01,  1.6066e-01],\n",
      "          [ 5.5362e-02,  3.3499e-02, -1.2796e-01, -7.2978e-02, -6.2339e-02],\n",
      "          [ 2.0375e-01,  4.3110e-02,  1.9312e-02,  8.7603e-02,  1.8687e-01]],\n",
      "\n",
      "         [[-5.6762e-01, -2.1872e-01, -2.7350e-01,  1.1480e+00,  3.3075e-01],\n",
      "          [ 1.1820e+00,  3.8943e-02,  3.0936e-01,  9.4286e-01,  7.3024e-02],\n",
      "          [ 3.5326e-01,  9.4350e-02,  9.3045e-01,  4.2749e-01,  1.5738e+00],\n",
      "          [-9.0817e-01, -3.8485e-01,  3.6207e-01,  7.2792e-01,  9.5045e-01],\n",
      "          [ 3.3897e-01,  5.8515e-01, -1.0482e+00, -1.2461e-01, -7.2011e-01]],\n",
      "\n",
      "         [[ 9.9446e-02,  1.4806e-01,  1.1216e-01,  1.2171e-03, -3.3709e-02],\n",
      "          [-1.0430e-01, -2.4730e-02, -4.3084e-02,  3.7862e-04,  1.8861e-02],\n",
      "          [-6.4757e-02, -7.5325e-02,  1.6717e-02, -5.7160e-03,  7.7274e-02],\n",
      "          [-5.9932e-02, -4.2461e-02,  8.8585e-02,  1.0630e-01, -6.5399e-02],\n",
      "          [ 1.7210e-02,  1.4709e-01, -5.6551e-02, -5.5935e-04,  2.3067e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1881e-01,  1.9765e-01,  1.8351e-01,  1.3034e-01,  4.0776e-01],\n",
      "          [ 2.5530e-01, -1.1284e-01,  4.5448e-01, -6.9470e-03, -3.1033e-01],\n",
      "          [ 1.2259e-02, -1.5203e-01,  9.1055e-02, -1.4280e-01, -4.0694e-01],\n",
      "          [ 4.0352e-03,  3.3572e-01,  1.3230e-01,  6.4363e-02, -7.8067e-02],\n",
      "          [-1.8031e-01, -4.1522e-02,  1.0673e-02, -2.7233e-01, -1.3242e-01]],\n",
      "\n",
      "         [[ 1.0179e+00,  4.1918e-01,  1.5931e+00, -7.4829e-01,  6.5911e-01],\n",
      "          [-8.6966e-01, -2.0522e-01,  6.0271e-02,  1.5656e-03,  4.3535e-01],\n",
      "          [-3.1445e-01,  3.1914e-01, -5.8175e-01,  1.0193e+00,  1.0099e+00],\n",
      "          [-8.6038e-01,  1.2332e+00,  2.6755e-01, -3.0481e-01,  3.3611e-01],\n",
      "          [ 1.1264e+00,  1.0858e+00, -5.6389e-01,  1.3577e-01,  1.2333e+00]],\n",
      "\n",
      "         [[ 2.2713e-03,  3.0817e-02,  1.0861e-01, -1.1159e-01,  7.0089e-02],\n",
      "          [-1.5160e-02, -1.4718e-02,  1.1054e-01,  9.5927e-03,  2.0696e-02],\n",
      "          [ 7.8402e-02, -6.0837e-02, -9.8994e-02, -8.4106e-02,  9.2494e-02],\n",
      "          [-2.6956e-02, -3.3057e-02,  5.2374e-02,  2.6235e-02,  4.2444e-02],\n",
      "          [-5.7492e-03, -8.3775e-02, -5.6519e-02,  9.9187e-02, -4.0888e-02]]]])\n",
      "****************************************************************************************************\n",
      "torch.Size([2, 5, 5])\n",
      "tensor([[[-0.8591, -0.0836, -0.6714,  1.2321,  0.4512],\n",
      "         [ 1.4357, -0.3886,  0.3445,  0.9884,  0.0459],\n",
      "         [ 0.0563, -0.0245,  0.8892,  0.5933,  1.8117],\n",
      "         [-0.9127, -0.3938,  0.3227,  0.7612,  0.8227],\n",
      "         [ 0.5599,  0.7753, -1.0854, -0.0376, -0.5102]],\n",
      "\n",
      "        [[ 1.1390,  0.6476,  1.8852, -0.7295,  1.1370],\n",
      "         [-0.6295, -0.3328,  0.6253,  0.0042,  0.1457],\n",
      "         [-0.2238,  0.1063, -0.5897,  0.7924,  0.6955],\n",
      "         [-0.8833,  1.5359,  0.4522, -0.2142,  0.3005],\n",
      "         [ 0.9404,  0.9605, -0.6097, -0.0374,  1.0600]]])\n"
     ]
    }
   ],
   "source": [
    "batch_gray_weighted = batch_weights.sum(-3)\n",
    "\n",
    "print(batch_weights.shape)\n",
    "print(batch_weights)\n",
    "\n",
    "print(\"*\" * 100)\n",
    "\n",
    "print(batch_gray_weighted.shape)\n",
    "print(batch_gray_weighted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145c705",
   "metadata": {},
   "source": [
    "## Broadcasting Case 2) Using torch.einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "95e2cf89",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5, 5]) torch.Size([3])\n",
      "tensor([[[ 1.5983, -0.3484,  0.5655,  0.3301,  1.1145],\n",
      "         [-0.4947, -0.7328,  0.3209, -0.0710,  1.2660],\n",
      "         [ 0.2566, -0.5516,  0.6858,  2.2369,  0.2389],\n",
      "         [-0.3658, -1.5018,  0.0093,  1.0337,  0.9548],\n",
      "         [-0.6740, -0.4026,  0.7824,  0.8234, -1.5986]],\n",
      "\n",
      "        [[ 0.4735, -0.2425, -0.8112,  0.0550, -0.2995],\n",
      "         [-0.4127, -0.4388, -1.1521, -0.8011,  0.3785],\n",
      "         [-1.8889, -2.7838,  0.1517,  0.6229, -0.4998],\n",
      "         [-0.0773,  1.6237,  1.0182, -0.3051,  1.2892],\n",
      "         [-0.8027, -0.0293, -1.7940,  0.5115, -0.1719]],\n",
      "\n",
      "        [[-0.7189,  0.9472,  0.8887,  0.1411, -0.3666],\n",
      "         [-0.6374,  1.1175, -1.8483, -0.2069, -0.4688],\n",
      "         [ 0.7709, -0.8909,  0.3729,  1.4639, -1.9464],\n",
      "         [ 0.6226,  1.8907,  0.4442,  0.9469,  0.2237],\n",
      "         [ 0.7057,  1.3127,  0.1154, -0.1406,  0.7674]]])\n",
      "tensor([0.2126, 0.7152, 0.0722])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray_weighted_fancy = torch.einsum('...chw,c->...hw', img_t, weights)\n",
    "\n",
    "print(img_t.shape, weights.shape)\n",
    "print(img_t)\n",
    "print(weights)\n",
    "\n",
    "img_gray_weighted_fancy == img_gray_weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08e1f24b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5, 5]) torch.Size([3])\n",
      "tensor([[[[-1.8390e+00, -6.1058e-02, -2.3992e+00,  3.8978e-01,  7.2499e-01],\n",
      "          [ 1.6839e+00, -1.8949e+00,  3.6814e-01,  2.1233e-01, -2.1607e-01],\n",
      "          [-1.0920e+00, -2.0470e-01, -2.7246e-01,  8.0659e-01,  7.5569e-01],\n",
      "          [ 2.6040e-01,  1.5757e-01, -6.0189e-01, -3.4326e-01, -2.9322e-01],\n",
      "          [ 9.5835e-01,  2.0278e-01,  9.0836e-02,  4.1205e-01,  8.7899e-01]],\n",
      "\n",
      "         [[-7.9365e-01, -3.0581e-01, -3.8241e-01,  1.6052e+00,  4.6246e-01],\n",
      "          [ 1.6527e+00,  5.4450e-02,  4.3255e-01,  1.3183e+00,  1.0210e-01],\n",
      "          [ 4.9393e-01,  1.3192e-01,  1.3010e+00,  5.9772e-01,  2.2005e+00],\n",
      "          [-1.2698e+00, -5.3810e-01,  5.0625e-01,  1.0178e+00,  1.3289e+00],\n",
      "          [ 4.7395e-01,  8.1816e-01, -1.4656e+00, -1.7423e-01, -1.0069e+00]],\n",
      "\n",
      "         [[ 1.3774e+00,  2.0508e+00,  1.5535e+00,  1.6857e-02, -4.6688e-01],\n",
      "          [-1.4446e+00, -3.4252e-01, -5.9673e-01,  5.2440e-03,  2.6123e-01],\n",
      "          [-8.9691e-01, -1.0433e+00,  2.3154e-01, -7.9168e-02,  1.0703e+00],\n",
      "          [-8.3009e-01, -5.8811e-01,  1.2269e+00,  1.4723e+00, -9.0581e-01],\n",
      "          [ 2.3837e-01,  2.0372e+00, -7.8326e-01, -7.7472e-03,  3.1948e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 5.5886e-01,  9.2966e-01,  8.6317e-01,  6.1309e-01,  1.9180e+00],\n",
      "          [ 1.2008e+00, -5.3078e-01,  2.1377e+00, -3.2677e-02, -1.4597e+00],\n",
      "          [ 5.7661e-02, -7.1509e-01,  4.2829e-01, -6.7170e-01, -1.9141e+00],\n",
      "          [ 1.8980e-02,  1.5791e+00,  6.2228e-01,  3.0274e-01, -3.6720e-01],\n",
      "          [-8.4814e-01, -1.9531e-01,  5.0201e-02, -1.2810e+00, -6.2288e-01]],\n",
      "\n",
      "         [[ 1.4232e+00,  5.8610e-01,  2.2274e+00, -1.0463e+00,  9.2158e-01],\n",
      "          [-1.2160e+00, -2.8694e-01,  8.4271e-02,  2.1891e-03,  6.0872e-01],\n",
      "          [-4.3967e-01,  4.4622e-01, -8.1341e-01,  1.4252e+00,  1.4121e+00],\n",
      "          [-1.2030e+00,  1.7243e+00,  3.7410e-01, -4.2618e-01,  4.6995e-01],\n",
      "          [ 1.5750e+00,  1.5182e+00, -7.8844e-01,  1.8983e-01,  1.7244e+00]],\n",
      "\n",
      "         [[ 3.1458e-02,  4.2683e-01,  1.5043e+00, -1.5455e+00,  9.7076e-01],\n",
      "          [-2.0997e-01, -2.0384e-01,  1.5310e+00,  1.3286e-01,  2.8665e-01],\n",
      "          [ 1.0859e+00, -8.4262e-01, -1.3711e+00, -1.1649e+00,  1.2811e+00],\n",
      "          [-3.7335e-01, -4.5785e-01,  7.2541e-01,  3.6337e-01,  5.8786e-01],\n",
      "          [-7.9628e-02, -1.1603e+00, -7.8281e-01,  1.3738e+00, -5.6631e-01]]]])\n",
      "tensor([0.2126, 0.7152, 0.0722])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]],\n",
       "\n",
       "        [[True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True],\n",
       "         [True, True, True, True, True]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_gray_weighted_fancy = torch.einsum('...chw,c->...hw', batch_t, weights)\n",
    "\n",
    "print(batch_t.shape, weights.shape)\n",
    "print(batch_t)\n",
    "print(weights)\n",
    "\n",
    "batch_gray_weighted_fancy == batch_gray_weighted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b86df",
   "metadata": {},
   "source": [
    "## Naming for Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5da2dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kck\\AppData\\Local\\Temp\\ipykernel_6808\\3843218446.py:1: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1408.)\n",
      "  weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels']) # Experimental(Not Stable)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.2126, 0.7152, 0.0722], names=('channels',))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_named = torch.tensor([0.2126, 0.7152, 0.0722], names=['channels']) # Experimental(Not Stable)\n",
    "weights_named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2db624d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img named : torch.Size([3, 5, 5]), ('channels', 'rows', 'columns')\n",
      "batch named : torch.Size([2, 3, 5, 5]), (None, 'channels', 'rows', 'columns')\n"
     ]
    }
   ],
   "source": [
    "img_named = img_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "batch_named = batch_t.refine_names(..., 'channels', 'rows', 'columns')\n",
    "\n",
    "print(f\"img named : {img_named.shape}, {img_named.names}\")\n",
    "print(f\"batch named : {batch_named.shape}, {batch_named.names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56f37415",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 1, 1]), ('channels', 'rows', 'columns'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The align_as() function fills in missing dimensions and converts existing dimensions to the correct order.\n",
    "weights_aligned = weights_named.align_as(img_named)\n",
    "\n",
    "weights_aligned.shape, weights_aligned.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ce1de57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), ('rows', 'columns'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_named = (img_named * weights_aligned).sum('channels')\n",
    "gray_named.shape, gray_named.names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1729b8f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 5]), (None, None))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_plain = gray_named.rename(None)\n",
    "gray_plain.shape, gray_plain.names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71163f5",
   "metadata": {},
   "source": [
    "## Tensor's dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aaebabd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float64 torch.int16\n"
     ]
    }
   ],
   "source": [
    "double_points = torch.ones(10, 2, dtype = torch.double)\n",
    "short_points = torch.tensor([[1, 2], [3, 4]], dtype = torch.short)\n",
    "print(double_points.dtype, short_points.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "94b948c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1) Converting data type\n",
    "double_points = torch.zeros(10, 2).double()\n",
    "short_points = torch.ones(10, 2).short()\n",
    "\n",
    "# Case 2) Converting data type\n",
    "double_points = torch.zeros(10, 2).to(torch.double)\n",
    "short_points = torch.ones(10, 2).to(dtype = torch.short)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b5dc01fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0.], dtype=torch.float64)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_64 = torch.rand(5, dtype = torch.double)\n",
    "points_short = points_64.to(torch.short)\n",
    "\n",
    "points_64 * points_short\n",
    "# When inputs with multiple types are mixed together through operation, the largest type is automatically generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1059e0d3",
   "metadata": {},
   "source": [
    "## Tensor Storage Indexing Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bce7479a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5aee98f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "points_storage = points.storage()\n",
    "print(points_storage[0])\n",
    "print(points.storage()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8a60c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([[2., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_storage = points.storage()\n",
    "print(points)\n",
    "\n",
    "points_storage[0] = 2.0\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0e9790",
   "metadata": {},
   "source": [
    "## Tensor Internal Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4502a3cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 2)\n",
    "\n",
    "'''\n",
    "It ends with an underscore (_) like zero_(), \n",
    "but instead of a new tensor being passed as a result of the operation, \n",
    "the existing tensor is also converted.\n",
    "'''\n",
    "\n",
    "print(a.zero_())\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a078c75",
   "metadata": {},
   "source": [
    "## Tensor Meta Data (size, offset, stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e65ff45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) torch.Size([3, 2])\n",
      "0\n",
      "(2, 1)\n",
      "**********\n",
      "torch.Size([2]) torch.Size([2])\n",
      "2\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1]\n",
    "\n",
    "print(points.size(), points.shape)\n",
    "print(points.storage_offset())\n",
    "print(points.stride())\n",
    "\n",
    "print(\"*\" * 10)\n",
    "\n",
    "print(second_point.size(), second_point.shape) # size() == shape()\n",
    "print(second_point.storage_offset())\n",
    "print(second_point.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7d1daec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([[ 4.,  1.],\n",
      "        [10.,  3.],\n",
      "        [ 2.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "second_point = points[1]    # Point the Same Tensor Storage\n",
    "print(points)\n",
    "second_point[0] = 10.0\n",
    "print(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "87947f5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([[4., 1.],\n",
      "        [5., 3.],\n",
      "        [2., 1.]])\n",
      "tensor([10.,  3.])\n"
     ]
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "second_point = points[1].clone()    # Clone New Tensor\n",
    "print(points)\n",
    "second_point[0] = 10.0\n",
    "print(points)\n",
    "print(second_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71554cee",
   "metadata": {},
   "source": [
    "## Tensor Transpose (Not Copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "855e4478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "43fb7fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t = points.t()\n",
    "points_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd803452",
   "metadata": {},
   "source": [
    "### Check about two tensor's id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7d1e0771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id(points.storage()) == id(points_t.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cb39ca03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1)\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(points.stride())\n",
    "print(points_t.stride())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf7cb586",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "torch.Size([5, 4, 3])\n",
      "(20, 5, 1)\n",
      "(1, 5, 20)\n"
     ]
    }
   ],
   "source": [
    "some_t = torch.ones(3, 4, 5)\n",
    "transpose_t = some_t.transpose(0, 2)\n",
    "print(some_t.shape)\n",
    "print(transpose_t.shape)\n",
    "print(some_t.stride())\n",
    "print(transpose_t.stride())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff43b92",
   "metadata": {},
   "source": [
    "## Contiguous Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "309280c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eee51315",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d7583",
   "metadata": {},
   "source": [
    "### Make Contiguous Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80d841ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.tensor([[4.0 ,1.0], [5.0, 3.0], [2.0, 1.0]])\n",
    "points_t = points.t()\n",
    "points_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9db64af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 4.0\n",
       " 1.0\n",
       " 5.0\n",
       " 3.0\n",
       " 2.0\n",
       " 1.0\n",
       "[torch.FloatStorage of size 6]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2b462394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d558955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 5., 2.],\n",
       "        [1., 3., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points_t_cont = points_t.contiguous()\n",
    "points_t_cont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ef238d62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 1)\n",
      " 4.0\n",
      " 5.0\n",
      " 2.0\n",
      " 1.0\n",
      " 3.0\n",
      " 1.0\n",
      "[torch.FloatStorage of size 6]\n"
     ]
    }
   ],
   "source": [
    "print(points_t_cont.stride())\n",
    "print(points_t_cont.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "01f37577",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4., 1.],\n",
       "        [5., 3.],\n",
       "        [2., 1.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5966c002",
   "metadata": {},
   "source": [
    "## Tensor Storage Space [CPU <-> GPU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "646eb264",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = torch.tensor([[4.0, 1.0], [5.0, 3.0], [2.0, 1.0]], device = 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fe63753",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points.to(device = 'cuda')\n",
    "\n",
    "# if gpu counts are more than one\n",
    "points_gpu = points.to(device = 'cuda:0')\n",
    "\n",
    "points = 2 * points    # Operation in CPU\n",
    "points_gpu = 2 * points.to(device = 'cuda')    # Operation in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34dfaa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points_gpu + 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "163dd9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_cpu = points_gpu.to(device = 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60e328a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "points_gpu = points.cuda()\n",
    "points_gpu = points.cuda(0)\n",
    "\n",
    "points_cpu = points_gpu.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d3d6d7",
   "metadata": {},
   "source": [
    "## Numpy Compatible (Numpy <-> Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4aac8f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.],\n",
       "       [1., 1., 1., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.ones(3, 4)\n",
    "\n",
    "points_np = points.numpy()    # Numpy -> Tensor\n",
    "points_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "918cf27c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points = torch.from_numpy(points_np)    # Tensor -> Numpy\n",
    "points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee19293",
   "metadata": {},
   "source": [
    "## Tensor Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ca489",
   "metadata": {},
   "source": [
    "### How to save when you plan to read only PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "042aefcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1) Save Tensor\n",
    "torch.save(points, '.\\\\serialization_1.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "083d451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2) Save Tensor\n",
    "\n",
    "with open('.\\\\serialization_2.t', 'wb') as f:\n",
    "    torch.save(points, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "05fcfc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1) File Load\n",
    "\n",
    "points = torch.load('.\\\\serialization_1.t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb838b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 2) File Load\n",
    "\n",
    "with open('.\\\\serialization_2.t', 'rb') as f:\n",
    "    points = torch.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e556eec5",
   "metadata": {},
   "source": [
    "### How to store tensor compatible when using multiple software -> Using h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "044d5159",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "992cb5fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 dataset \"coords\": shape (3, 4), type \"<f4\">\n"
     ]
    }
   ],
   "source": [
    "f = h5py.File('.\\\\h5py_tensor.hdf5', 'w')\n",
    "dset = f.create_dataset('coords', data = points.numpy())\n",
    "print(dset)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "edcaf9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('.\\\\h5py_tensor.hdf5', 'r')\n",
    "dset = f['coords']\n",
    "last_points = dset[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c8d1e91a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]) torch.float32\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "<Closed HDF5 dataset>\n"
     ]
    }
   ],
   "source": [
    "last_points = torch.from_numpy(dset[-2:])\n",
    "print(last_points, last_points.dtype)\n",
    "\n",
    "f.close()\n",
    "\n",
    "print(last_points)\n",
    "print(dset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
