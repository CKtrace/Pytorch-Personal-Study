{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef4d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde4eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "data_path = \".\\\\CIFAR10\\\\\"\n",
    "\n",
    "cifar10 = datasets.CIFAR10(\n",
    "    data_path, train = True, download = False, \n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=False,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),(0.2470, 0.2435, 0.2616))\n",
    "    ]))\n",
    "\n",
    "\n",
    "label_map = {0: 0, 2: 1}\n",
    "class_names = ['airplane', 'bird']\n",
    "\n",
    "cifar2 = [(img, label_map[label])\n",
    "          for img, label in cifar10 \n",
    "          if label in [0, 2]]\n",
    "\n",
    "cifar2_val = [(img, label_map[label])\n",
    "              for img, label in cifar10_val\n",
    "              if label in [0, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0e79f7",
   "metadata": {},
   "source": [
    "# Model Memory Efficient    (Soft Coding > Hard Coding)\n",
    "\n",
    "```\n",
    "각 계층에서 채널과 피처의 수를 나타내는 숫자들은 직접적으로 모델의 파라미터 수에 영향을 주기에 동일 모델 선언 시 Soft Coding의 경우 모델의 용량을 감소시킨다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88d19dff",
   "metadata": {},
   "source": [
    "## Hard Coding    ->    일일이 파라미터 값을 부여"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdb73ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 16, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(16 * 8 * 8, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 16 * 8 * 8)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5125a4ae",
   "metadata": {},
   "source": [
    "## Soft Coding    ->   n_chans1 변수에 저장된 값을 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fafc00c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetWidth(nn.Module):\n",
    "    def __init__(self, n_chans1 = 32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(8 * 8 * self.n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fcd4f1",
   "metadata": {},
   "source": [
    "# Prevention Method of Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b560a1f",
   "metadata": {},
   "source": [
    "## 1. Add Regularization term (ex. L1, L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38dad45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "\n",
    "print(f\"Training on device {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f36fc389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "    \n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print(f\"{datetime.datetime.now()} Epoch {epoch}, Training Loss {loss_train / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0dbc543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-14 18:39:47.241464 Epoch 1, Training Loss 0.5816713928417035\n",
      "2023-01-14 18:39:52.197775 Epoch 10, Training Loss 0.3492789014129882\n",
      "2023-01-14 18:39:57.672336 Epoch 20, Training Loss 0.3076192063700621\n",
      "2023-01-14 18:40:03.121422 Epoch 30, Training Loss 0.2772480789453361\n",
      "2023-01-14 18:40:08.594407 Epoch 40, Training Loss 0.25395985080558026\n",
      "2023-01-14 18:40:14.080248 Epoch 50, Training Loss 0.236167131335872\n",
      "2023-01-14 18:40:19.617275 Epoch 60, Training Loss 0.21687189423164743\n",
      "2023-01-14 18:40:25.501859 Epoch 70, Training Loss 0.20440235278408997\n",
      "2023-01-14 18:40:30.951893 Epoch 80, Training Loss 0.1896446351982226\n",
      "2023-01-14 18:40:36.433873 Epoch 90, Training Loss 0.1755782100520316\n",
      "2023-01-14 18:40:41.917731 Epoch 100, Training Loss 0.16544538195345812\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size = 64, shuffle = True)\n",
    "\n",
    "model = NetWidth().to(device = device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5953c10",
   "metadata": {},
   "source": [
    "## 2. Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ac659e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetDropout(nn.Module):\n",
    "    def __init__(self, n_chans1 = 32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_dropout = nn.Dropout2d(p = 0.4)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_dropout = nn.Dropout2d(p = 0.4)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
    "        out = self.conv1_dropout(out)\n",
    "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
    "        out = self.conv2_dropout(out)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607098d8",
   "metadata": {},
   "source": [
    "## 3. Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6511d47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm(nn.Module):\n",
    "    def __init__(self, n_chans1 = 32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv1_batchnorm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv2_batchnorm = nn.BatchNorm2d(num_features=n_chans1 // 2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1_batchnorm(self.conv1(x))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = self.conv2_batchnorm(self.conv2(out))\n",
    "        out = F.max_pool2d(torch.tanh(out), 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1 // 2)\n",
    "        out = torch.tanh(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c37ed02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-14 19:30:44.712239 Epoch 1, Training Loss 0.5604936668447628\n",
      "2023-01-14 19:30:50.042916 Epoch 10, Training Loss 0.3402536412712875\n",
      "2023-01-14 19:30:55.569086 Epoch 20, Training Loss 0.3031249007411823\n",
      "2023-01-14 19:31:01.168389 Epoch 30, Training Loss 0.2760226851815631\n",
      "2023-01-14 19:31:06.689887 Epoch 40, Training Loss 0.2554647217795348\n",
      "2023-01-14 19:31:12.172260 Epoch 50, Training Loss 0.2400734574551795\n",
      "2023-01-14 19:31:17.659029 Epoch 60, Training Loss 0.22504083294967178\n",
      "2023-01-14 19:31:23.145827 Epoch 70, Training Loss 0.20992670654301432\n",
      "2023-01-14 19:31:28.627116 Epoch 80, Training Loss 0.19661309513126968\n",
      "2023-01-14 19:31:34.125183 Epoch 90, Training Loss 0.18382295097704907\n",
      "2023-01-14 19:31:39.624311 Epoch 100, Training Loss 0.17265565793035895\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(cifar2, batch_size = 64, shuffle = True)\n",
    "\n",
    "model = NetWidth().to(device = device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 100,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c74f6b",
   "metadata": {},
   "source": [
    "# For Complex Structure    ->    Skip Connection\n",
    "\n",
    "```\n",
    "매우 깊은 심층 신경망에서 역전파 진행 시 손실값과 파라미터 사이의 미분 연산의 체인 연결에서 오는 많은 수로 곱해져야 하고, 매우 작은 값이나 매우 큰 값이 생성되거나 부동소수점 근사 과정에서 작은 값들 소멸될 수도 있다.\n",
    "\n",
    "즉, Gradient Vanishing 문제가 발생될 수 있으며, 이를 해결해주는 것이 바로, Skip Connection이다.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4da140d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetRes(nn.Module):\n",
    "    def __init__(self, n_chans1 = 32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out1 = out\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out) + out1), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9350fa9",
   "metadata": {},
   "source": [
    "## Using Building Block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2083a843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans1):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans1, n_chans1, kernel_size=3, padding=1, bias=False)\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans1)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight, nonlinearity='relu')\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d1c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1 = 32, n_blocks = 10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans1 = n_chans1)]))\n",
    "        self.fc1 = nn.Linear(-1, 8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
