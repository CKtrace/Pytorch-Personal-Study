# Regularization

기본적으로 정규화를 하는 이유는 두 가지이다.

1. 학습 속도 향상
2. 최대값 혹은 최대값을 갖지 않는 극값에 빠지는 가능성 감소

## 1. Weight Penalty

```
모델 일반화를 위해 손실값에 조작된 정규화 항을 넣어 모델의 가중치가 상대적으로 작게 만든다.

=> 훈련을 통해 증가할 수 있는 크기 제한, 큰 가중치 값에 페널티 부과

ex) L1 Regularization, L2 Regularization
```

## 2. Dropout

```
훈련을 반복할 때마다 신경망의 출력을 랜덤하게 0으로 만드는 작업

본 방법 사용 시, 매 훈련 때마다 조금씩 다른 뉴런의 토플로지가 생성되어 신경망이 각 입력 샘플을 암기하려는 기회 감소 ->  Overfit 방지
```

## 3. Batch Normalization

```
학습 과정에서 계층 별로 입력의 데이터 분포가 달라지기 때문에 다양한 분포를 가지더라도 각 배치별로 평균과 분산을 이용해 정규화하는 것

쉽게 말하자면, 배치 별로 어떤 데이터가 들어갈지 모르기 때문에 (예를 들어, 한 쪽으로 쏠린 데이터들만 들어가는 경우) 하는 작업

Data Scaling 작업과 유사하다고 생각하면 이해가 더욱 쉽다.
```